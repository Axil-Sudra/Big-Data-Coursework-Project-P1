{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x3aJEGS5keqH"
   },
   "source": [
    "# Coursework Part 1: Detecting Spam with Spark\n",
    "\n",
    "These are the tasks for IN432 Big Data coursework 2019, part 1.  \n",
    "\n",
    "This coursework is about classification of e-mail messages as spam or non-spam in Spark. We will go through the whole process from loading and preprocessing to training and testing classifiers in a distributed way in Spark. We wil use the techniques shown in the lectures and labs. I will also introduce here a few additional elements, such as the NLTK and some of the preprocessing and machine learning functions that come with Spark. You are not expected to need anything beyond the material handed out so far and in some cases the Spark documentation, to which I have put links in this document.  \n",
    "\n",
    "The structure is similar to the lab sheets. I provide a code structure with gaps that you are supposed to file. In addition you should run 2 small experiments and comment on the results. The lines where you are supposed to add code or take another action are marked with \">>>\" \n",
    "please leave the \">>>\" in the text, comment out that line, and write your own code in the next line using a copy of that line as a starting point.\n",
    "\n",
    "I have added numerous comments in text cells and the code cells to guide you through the program. Please read them carefully and ask if anything is unclear. \n",
    "\n",
    "Once you have completed the tasks, don't delete the outpus, but download the notebook (outputs will be included) and upload it into the coursework submission area on Moodle. The coursework part counts for 50% or the total coursework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CdFlCqCFkeqL"
   },
   "source": [
    "## Load and prepare the data\n",
    "\n",
    "We will use the lingspam dataset in this coursework (see [http://csmining.org/index.php/ling-spam-datasets.html](http://csmining.org/index.php/ling-spam-datasets.html) for more information).\n",
    "\n",
    "The next cells only prepare the machine, as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "dHGQ78mTkeqO",
    "outputId": "64919987-a498-424d-8621-6e3a10092e6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "0-hhNOS0keqW",
    "outputId": "701fbd85-095f-41c1-aec0-55485d34d674"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/01/a37e827c2d80c6a754e40e99b9826d978b55254cc6c6672b5b08f2e18a7f/pyspark-2.4.0.tar.gz (213.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 213.4MB 130kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "\u001b[K    100% |████████████████████████████████| 204kB 31.1MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/cd/54/c2/abfcc942eddeaa7101228ebd6127a30dbdf903c72db4235b23\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 656
    },
    "colab_type": "code",
    "id": "nncrHFdwqUmE",
    "outputId": "6d37d413-3cf2-4166-e58d-4f64d19563bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/BigData/data/lingspam_public\n",
      "This directory contains the Ling-Spam corpus, as described in the \n",
      "paper:\n",
      "\n",
      "I. Androutsopoulos, J. Koutsias, K.V. Chandrinos, George Paliouras, \n",
      "and C.D. Spyropoulos, \"An Evaluation of Naive Bayesian Anti-Spam \n",
      "Filtering\". In Potamias, G., Moustakis, V. and van Someren, M. (Eds.), \n",
      "Proceedings of the Workshop on Machine Learning in the New Information \n",
      "Age, 11th European Conference on Machine Learning (ECML 2000), \n",
      "Barcelona, Spain, pp. 9-17, 2000.\n",
      "\n",
      "There are four subdirectories, corresponding to four versions of \n",
      "the corpus:\n",
      "\n",
      "bare: Lemmatiser disabled, stop-list disabled.\n",
      "lemm: Lemmatiser enabled, stop-list disabled.\n",
      "lemm_stop: Lemmatiser enabled, stop-list enabled.\n",
      "stop: Lemmatiser disabled, stop-list enabled.\n",
      "\n",
      "Each one of these 4 directories contains 10 subdirectories (part1, \n",
      "..., part10). These correspond to the 10 partitions of the corpus \n",
      "that were used in the 10-fold experiments. In each repetition, one \n",
      "part was reserved for testing and the other 9 were used for training. \n",
      "\n",
      "Each one of the 10 subdirectories contains both spam and legitimate \n",
      "messages, one message in each file. Files whose names have the form\n",
      "spmsg*.txt are spam messages. All other files are legitimate messages.\n",
      "\n",
      "By obtaining a copy of this corpus you agree to acknowledge the use \n",
      "and origin of the corpus in any published work of yours that makes \n",
      "use of the corpus, and to notify the person below about this work.\n",
      "\n",
      "Ion Androutsopoulos \n",
      "http://www.aueb.gr/users/ion/\n",
      "Ling-Spam corpus last updated: July 17, 2000\n",
      "This file (readme.txt) last updated: July 30, 2003.\n"
     ]
    }
   ],
   "source": [
    "# We have a new dataset in directory BigData/data/lingspam_public .\n",
    "%cd /content/drive/My Drive/BigData/data/lingspam_public \n",
    "# the line above should output should show \"bare  lemm  lemm_stop  readme.txt  stop\"\n",
    "!cat readme.txt\n",
    "# the line above shows the content of the readme file, which explains the structrue of the dataset\n",
    "# Lemmatisation is a process similar to stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nb02XcMOkeqq"
   },
   "source": [
    "## Task 1) Read the dataset and create RDDs \n",
    "a) Start by reading the directory with text files from the file system (`/content/drive/My Drive/BigData/data/lingspam_public`). Load all text files per directory (part1,part2, ... ,part10) using `wholeTextFiles()`, which creates one RDD per part, containing tuples (filename,text). This is a good choice as the text files are small. (5%)\n",
    "\n",
    "b) We will use one of the RDDs as test set, the rest as training set. For the training set you need to create the union of the remaining RDDs. (5%)\n",
    "\n",
    "b) Remove the path and extension from the filename using the regular expression provided (5%).\n",
    "\n",
    "If the filename starts with 'spmsg' it is spam, otherwise it is not. We'll use that later to train a classifier. \n",
    "\n",
    "We will put the code in each cell into a function that we can reuse later. In this way we can develop the whole preprocessing with the smaller test set and apply it to the training set once we know that everything works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "H7iF1lZukeqt",
    "outputId": "bcc9f0ef-dab3-4cd5-a0b1-1d3b712f92b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/BigData/data/lingspam_public\n",
      "bare  lemm  lemm_stop  readme.txt  stop\n",
      "[PosixPath('bare/part10'), PosixPath('bare/part9'), PosixPath('bare/part8'), PosixPath('bare/part7'), PosixPath('bare/part6'), PosixPath('bare/part5'), PosixPath('bare/part4'), PosixPath('bare/part3'), PosixPath('bare/part2'), PosixPath('bare/part1')]\n",
      "len(rddList) 10\n",
      "[('file:/content/drive/My Drive/BigData/data/lingspam_public/bare/part9/8-817msg1.txt', 'Subject: disc : grammar in uk schools\\n\\ni was disturbed to find some traditional fallacies in geoffrey sampson \\'s discussion of the teaching of grammar in schools . though i no longer have a copy of prof . cameron \\'s original post , i do recall the essentials of it , and found that mr . sampson had passed over the valid point it was making in favour of a prescriptivist , \" back to basics \" defense of traditional grammatical education . prof . cameron is perfectly correct in ridiculing the inflexible , rote and prescriptive approach to grammar which is conventionally inflicted on students throughout the english speaking world . the issue of being able to use standard english ( or perhaps _ a _ standard english ) correctly is entirely separate from the reliance on traditional \" rules \" which are frequently unhelpful , and often grossly inaccurate . the rule regarding finishing sentences with prepositions , as one glaring example , is a total misunderstanding of both the history of english , and an unhelpful preoccupation for effective communication . > strikes me as akin to suggesting that teachers of > french should forget about teaching the past participle of \" vivre \" in > favour of getting their pupils to develop considered opinions about > the theories of derrida . though mr . sampson has used an interesting rhetorical image here , it is in fact a false analogy . teaching students to get a feel for the function of grammar and language is a far cry from teaching them gb theory or hpsg . an understanding of how sentences , clauses , verb tenses , adverbs , etc . actually function on a basic level is a very reasonable educational goal , and far more worthy then just creating a bunch of \" do n\\'t \" \\'s and \" never \" \\'s and calling that grammatical education . > beyond that , though , teaching orthography and grammar at school level > has a much broader educational value . one of the lessons we all have > to learn is that nothing big and worthwhile is ever achieved in this > life without careful attention to endless tedious and often arbitrary > details . at the risk of making a gross national stereotype , i feel compelled to quote george bernard shaw : \" the british believe that they are moral when they are merely uncomfortable . \" this notion that education ( or work , for that matter ) must be unpleasant to produce results is a puritanical relic . in my personal experience , the very successful people tend to be precisely the ones who know how to delegate , slough off or avoid wasting time with \" tedious and arbitrary details \" . ( please read the preceding paragraph with the tongue planted in the general vicinity of the cheek . ) in a spirit of greater seriousness though , i would like to second prof . cameron \\'s call to educators to abandon prescriptive , rule-based approaches to grammar , and embrace a more general approach based on a comprehension of more fluid and meaningful principles . i believe that the result would be students with a better grasp of the form and function of language rather than a shallow and inflexible mastery of facile rules . - - - - marc hamann\\n')]\n",
      "created the RDDs\n",
      "testRDD.count():  289\n",
      "testRDD.getNumPartitions() 2\n",
      "testRDD.getStorageLevel() Serialized 1x Replicated\n",
      "testRDD.take(1):  [('3-1msg1', 'Subject: re : 2 . 882 s - > np np\\n\\n> date : sun , 15 dec 91 02 : 25 : 02 est > from : michael < mmorse @ vm1 . yorku . ca > > subject : re : 2 . 864 queries > > wlodek zadrozny asks if there is \" anything interesting \" to be said > about the construction \" s > np np \" . . . second , > and very much related : might we consider the construction to be a form > of what has been discussed on this list of late as reduplication ? the > logical sense of \" john mcnamara the name \" is tautologous and thus , at > that level , indistinguishable from \" well , well now , what have we here ? \" . to say that \\' john mcnamara the name \\' is tautologous is to give support to those who say that a logic-based semantics is irrelevant to natural language . in what sense is it tautologous ? it supplies the value of an attribute followed by the attribute of which it is the value . if in fact the value of the name-attribute for the relevant entity were \\' chaim shmendrik \\' , \\' john mcnamara the name \\' would be false . no tautology , this . ( and no reduplication , either . )\\n')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def makeTestTrainRDDs(pathString):\n",
    "    \"\"\" Takes one of the four subdirectories of the lingspam dataset and returns two RDDs one each for testing and training. \"\"\"\n",
    "    # We should see 10 parts that we can use for creating train and test sets.\n",
    "    p = Path(pathString) # gets a path object representing the current directory path.\n",
    "    dirs = list(p.iterdir()) # get the directories part1 ... part10. \n",
    "    print(dirs) # Print to check that you have the right directory. You can comment this out when checked. \n",
    "    rddList = [] # create a list for the RDDs\n",
    "    # now create an RDD for each 'part' directory and add them to rddList\n",
    "    for d in dirs: # iterate through the directories\n",
    "#>>>     rdd = ... #>>> # read the files in the directory\n",
    "         rdd = sc.wholeTextFiles(str(d.absolute())) # read the files in the directory\n",
    "#>>>     ... #>>> append the RDD to the rddList\n",
    "         rddList.append(rdd) # append the RDD to the rddList\n",
    "    print('len(rddList)',len(rddList))  # we should now have 10 RDDs in the list # just for testing\n",
    "    print(rddList[1].take(1)) # just for testing, comment out when it works.\n",
    "    testRDD1 = rddList[9] # set the test set\n",
    "    trainRDD1 = rddList[0] # start the training set from 0 and \n",
    "    # now loop over the range from 1 to 9(exclusive) to create a union of the remaining RDDs\n",
    "    for i in range(1,9):\n",
    "        trainRDD1 = trainRDD1.union(rddList[i]) #>>> create a union of the current and the next \n",
    "            # RDD in the list, so that in the end we have a union of all parts 0-8. (9 ist used as test set)\n",
    "    # both RDDs should remove the paths and extensions from the filename. \n",
    "    #>>> This regular expression will do it: re.split('[/\\.]', fn_txt[0])[-2]\n",
    "    #>>> apply it to the filenames in train and test RDD with a lambda\n",
    "#>>>    testRDD2 = testRDD1.map(lambda ...) \n",
    "        testRDD2 = testRDD1.map(lambda fn_txt: (re.split('[/\\.]', fn_txt[0])[-2], fn_txt[1])) # testRDD2 = testRDD1.map(lambda ...)\n",
    "#>>>    trainRDD2 = trainRDD1.map(lambda ...)\n",
    "        trainRDD2 = trainRDD1.map(lambda fn_txt: (re.split('[/\\.]', fn_txt[0])[-2], fn_txt[1])) # trainRDD2 = trainRDD1.map(lambda ...)\n",
    "    return (trainRDD2,testRDD2)\n",
    "\n",
    "# this makes sure we are in the right directory\n",
    "%cd /content/drive/My Drive/BigData/data/lingspam_public \n",
    "# this should show \"bare  lemm  lemm_stop  readme.txt  stop\"\n",
    "!ls\n",
    "# the code below is for testing the function makeTestTrainRDDs\n",
    "trainRDD_testRDD = makeTestTrainRDDs('bare') # read from the 'bare' directory - this takes a bit of time\n",
    "(trainRDD,testRDD) = trainRDD_testRDD # unpack the returned tuple\n",
    "print('created the RDDs') # notify the user, so that we can figure out where things went wrong if they do.\n",
    "print('testRDD.count(): ',testRDD.count()) # should be ~291 \n",
    "#print('trainRDD.count(): ',trainRDD.count()) # should be ~2602 - commented out to save time as it takes some time to create RDD from all the files\n",
    "print('testRDD.getNumPartitions()',testRDD.getNumPartitions()) # normally 2 on Colab (single machine)\n",
    "print('testRDD.getStorageLevel()',testRDD.getStorageLevel()) # Serialized, 1x Replicated, expected to be (False, False, False, False, 1) \n",
    "print('testRDD.take(1): ',testRDD.take(1)) # should be (filename,[tokens]) \n",
    "rdd1 = testRDD # use this for developemnt in the next tasks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUKltxq5keqw"
   },
   "source": [
    "## Task 2) Tokenize and remove punctuation\n",
    "\n",
    "Now we need to split the words, a process called *tokenization* by linguists, and remove punctuation. \n",
    "\n",
    "We will use the Python [Natural Language Toolkit](http://www.nltk.org) *NLTK* to do the tokenization (rather than splitting ourselves, as these specialist tools usually do that better than we can ourselves). We use the NLTK function word_tokenize, see here for a code example: [http://www.nltk.org/book/ch03.html](http://www.nltk.org/book/ch03.html). 5%\n",
    "\n",
    "Then we will remove punctuation. There is no specific funtion for this, so we use a regular expression (see here for info [https://docs.python.org/3/library/re.html?highlight=re#module-re](https://docs.python.org/3/library/re.html?highlight=re#module-re)) in a list comprehension (here's a nice visual explanation: [http://treyhunner.com/2015/12/python-list-comprehensions-now-in-color/](http://treyhunner.com/2015/12/python-list-comprehensions-now-in-color/)). 5% \n",
    "\n",
    "We use a new technique here: we separate keys and values of the RDD, using the RDD functions `keys()` and `values()`, which yield each a new RDD. Then we process the values and *zip* them together with the keys again. See here for documentation: [http://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD.zip](http://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD.zip).  We wrap the whole sequence into one function `prepareTokenRDD` for later use. 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "AB_nfmhYkeqx",
    "outputId": "0a66a277-ff7c-4e3a-ba82-9a113b3bb3db",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('3-1msg1', ['Subject', ':', 're', ':', '2', '882', 's', '-', '>', 'np', 'np', '>', 'date', ':', 'sun', '15', 'dec', '91', '02', ':', '25', ':', '02', 'est', '>', 'from', ':', 'michael', '<', 'mmorse', '@', 'vm1', 'yorku', 'ca', '>', '>', 'subject', ':', 're', ':', '2', '864', 'queries', '>', '>', 'wlodek', 'zadrozny', 'asks', 'if', 'there', 'is', '``', 'anything', 'interesting', '``', 'to', 'be', 'said', '>', 'about', 'the', 'construction', '``', 's', '>', 'np', 'np', '``', 'second', '>', 'and', 'very', 'much', 'related', ':', 'might', 'we', 'consider', 'the', 'construction', 'to', 'be', 'a', 'form', '>', 'of', 'what', 'has', 'been', 'discussed', 'on', 'this', 'list', 'of', 'late', 'as', 'reduplication', 'the', '>', 'logical', 'sense', 'of', '``', 'john', 'mcnamara', 'the', 'name', '``', 'is', 'tautologous', 'and', 'thus', 'at', '>', 'that', 'level', 'indistinguishable', 'from', '``', 'well', 'well', 'now', 'what', 'have', 'we', 'here', '``', 'to', 'say', 'that', \"'\", 'john', 'mcnamara', 'the', 'name', \"'\", 'is', 'tautologous', 'is', 'to', 'give', 'support', 'to', 'those', 'who', 'say', 'that', 'a', 'logic-based', 'semantics', 'is', 'irrelevant', 'to', 'natural', 'language', 'in', 'what', 'sense', 'is', 'it', 'tautologous', 'it', 'supplies', 'the', 'value', 'of', 'an', 'attribute', 'followed', 'by', 'the', 'attribute', 'of', 'which', 'it', 'is', 'the', 'value', 'if', 'in', 'fact', 'the', 'value', 'of', 'the', 'name-attribute', 'for', 'the', 'relevant', 'entity', 'were', \"'\", 'chaim', 'shmendrik', \"'\", \"'\", 'john', 'mcnamara', 'the', 'name', \"'\", 'would', 'be', 'false', 'no', 'tautology', 'this', 'and', 'no', 'reduplication', 'either'])]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\" Apply the nltk.word_tokenize() method to our text, return the token list. \"\"\"\n",
    "    nltk.download('punkt') # this loads the standard NLTK tokenizer model \n",
    "    # it is important that this is done here in the function, as it needs to be done on every worker.\n",
    "    # If we do the download outside a this function, it would only be executed on the driver     \n",
    "#>>>    return ... # use the nltk function word_tokenize\n",
    "    return nltk.word_tokenize(text) # use the nltk function word_tokenize\n",
    "    \n",
    "def removePunctuation(tokens):\n",
    "    \"\"\" Remove punctuation characters from all tokens in a provided list. \"\"\"\n",
    "    # this will remove all punctiation from string s: re.sub('[()\\[\\],.?!\";_]','',s)\n",
    "#>>>    tokens2 =  [...] # use a list comprehension to remove punctuaton\n",
    "    tokens2 = [re.sub('[()\\[\\],.?!\";_]','',s) for s in tokens] # tokens2 = [...] # use a list comprehension to remove punctuation\n",
    "    return tokens2\n",
    "    \n",
    "def prepareTokenRDD(fn_txt_RDD):\n",
    "    \"\"\" Take an RDD with (filename,text) elements and transform it into a (filename,[token ...]) RDD without punctuation characters. \"\"\"\n",
    "    rdd_vals2 = fn_txt_RDD.values() # It's convenient to process only the values. \n",
    "    rdd_vals3 = rdd_vals2.map(tokenize) # Create a tokenised version of the values by mapping\n",
    "    rdd_vals4 = rdd_vals3.map(removePunctuation) # remove punctuation from the values\n",
    "    rdd_kv = fn_txt_RDD.keys().zip(rdd_vals4) # we zip the two RDDs together \n",
    "    # i.e. produce tuples with one item from each RDD.\n",
    "    # This works because we have only applied mappings to the values, \n",
    "    # therefore the items in both RDDs are still aligned.\n",
    "    # now remove any empty value strings (i.e. length 0) that we may have created by removing punctiation.\n",
    "    # >>> now remove any empty strings (i.e. length 0) that we may have \n",
    "    # created by removing punctuation, and resulting entries without words left.\n",
    "    # rdd_kvr = rdd_kv. # remove empty strings using RDD.map and a lambda. TIP len(s) gives you the lenght of string. \n",
    "    rdd_kvr = rdd_kv.map(lambda x: (x[0], [s for s in x[1] if len(s)>0]))  # remove empty strings using RDD.map and a lambda. TIP len(s) gives you the lenght of string.\n",
    "    # rdd_kvrf = rdd_kvr. # remove items without tokens using RDD.filter and a lambda. \n",
    "    rdd_kvrf = rdd_kvr.filter(lambda x: len(x[0]) > 0) # remove items without tokens using RDD.filter and a lambda.\n",
    "    # >>> Question: why should this be filtering done after zipping the keys and values together?\n",
    "    # Solution: To maintain the mapping between the values and their corresponding primary keys.  \n",
    "    return rdd_kvrf \n",
    "\n",
    "rdd2 = prepareTokenRDD(rdd1) # Use a small RDD for testing.\n",
    "print(rdd2.take(1)) # For checking result of task 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9HgRAEGkeq0"
   },
   "source": [
    "## Task 3) Creating normalised TF.IDF vectors of defined dimensionality, measure the effect of caching.\n",
    "\n",
    "We use the hashing trick to create fixed size TF vectors directly from the word list now (slightly different from the previous lab, where we used *(word,count)* pairs.). Write a bit of code as needed. (5%)\n",
    "\n",
    "Then we'll use the IDF and Normalizer functions provided by Spark. They use a slightly different pattern than RDD.map and reduce, have a look at the examples here in the documentation for Normalizer  and IDF:\n",
    "[http://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.feature.Normalizer](http://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.feature.Normalizer), [http://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.feature.IDF](http://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.feature.IDF) (5%)\n",
    "\n",
    "We want control of the dimensionality in the `normTFIDF` function, so we introduce an argument into our functions that enables us to vary dimensionalty later. Here is also an opportunity to benefit from caching, i.e. persisting the RDD after use, so that it will not be recomputed.  (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Qo_YQKAokeq0",
    "outputId": "d8b7f6c1-68fd-460c-9ed3-907e5476e9fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('3-1msg1', DenseVector([0.0, 0.1629, 0.6826, 0.0, 0.0, 0.0, 0.4017, 0.3258, 0.3133, 0.3766]))]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# use the hashing trick to create a fixed-size vector from a word list\n",
    "def hashing_vectorize(text,N): # arguments: the list and the size of the output vector\n",
    "    v = [0] * N  # create vector of 0s\n",
    "    for word in text: # iterate through the words \n",
    "#>>>              # get the hash value \n",
    "      h = hash(word) # get the hash value\n",
    "#>>>              # add 1 at the hashed address \n",
    "      v[h % N] = v[h % N] + 1 # add 1 at the hashed address\n",
    "    return v # return hashed word vector\n",
    "\n",
    "from pyspark.mllib.feature import IDF, Normalizer\n",
    "\n",
    "def normTFIDF(fn_tokens_RDD, vecDim, caching=True):\n",
    "    keysRDD = fn_tokens_RDD.keys()\n",
    "    tokensRDD = fn_tokens_RDD.values()\n",
    "    tfVecRDD = tokensRDD.map(lambda tokens: hashing_vectorize(tokens,vecDim)) \n",
    "    if caching:\n",
    "        tfVecRDD.persist(StorageLevel.MEMORY_ONLY) # since we will read more than once, caching in Memory will make things quicker.\n",
    "    idf = IDF() # create IDF object\n",
    "    idfModel = idf.fit(tfVecRDD) # calculate IDF values\n",
    "    tfIdfRDD = idfModel.transform(tfVecRDD) # 2nd pass needed (see lecture slides), transforms RDD\n",
    "#>>>    norm = ... # create a Normalizer object like in the example linked above\n",
    "    norm = Normalizer() # create a Normalizer object like in the example linked above\n",
    "#>>>    normTfIdfRDD = norm. ... # and apply it to the tfIdfRDD\n",
    "    normTfIdfRDD = norm.transform(tfIdfRDD) # and apply it to the tfIdfRDD\n",
    "#>>>    zippedRDD = ... # zip the keys and values together\n",
    "    zippedRDD = keysRDD.zip(normTfIdfRDD) # zip the keys and values together\n",
    "    return zippedRDD\n",
    "\n",
    "testDim = 10 # too small for good accuracy, but OK for testing\n",
    "rdd3 = normTFIDF(rdd2, testDim, True) # test our\n",
    "print(rdd3.take(1)) # we should now have tuples with ('filename',[N-dim vector])\n",
    "# e.g. [('9-1142msg1', DenseVector([0.0, 0.0, 0.0, 0.0, 0.4097, 0.0, 0.0, 0.0, 0.9122, 0.0]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "datJXbSNkeq2"
   },
   "source": [
    "### Task 3a) Caching experiment\n",
    "\n",
    "The normTFIDF lets us switch caching on or off. Write a bit of code that measures the effect of caching by takes the time for both options. Use the time function as shown in lecture 3, slide 47. Remember that you need to call an action on an RDD to trigger full execution. \n",
    "\n",
    "Add a short comment on the result (why is there an effect, why of the size that it is?). Remember that this is wall clock time, i.e. you may get noisy results and they may change depending on the system state (e.g. how often this test has been run). (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tdzLL4A_keq3",
    "outputId": "50222eb6-41df-4b06-923b-d175d596e3e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF.IDF vectors, 3 trials - mean time with caching:  17.605319897333782 , mean time without caching:  23.110472758611042\n"
     ]
    }
   ],
   "source": [
    "#run a small experiment with caching set to True or False, 3 times each\n",
    "\n",
    "from time import time\n",
    "\n",
    "resCaching = [] # for storing results\n",
    "resNoCache = [] # for storing results\n",
    "for i in range(3): # 3 samples\n",
    "#>>>  # start timer\n",
    "    startTime = time() # start timer\n",
    "    testRDD1 = normTFIDF(rdd2, testDim, True) # \n",
    "#>>>     # call an action on the RDD to trigger execution\n",
    "    testRDD1.count() # call an action on the RDD to trigger execution (in this case count() has been used as the action)\n",
    "#>>>  # end timer\n",
    "    endTime = time() # end time\n",
    "    resCaching.append( endTime - startTime ) # calculate the time spent\n",
    "    \n",
    "for i in range(3): # 3 samples\n",
    "#>>>  # start timer\n",
    "    startTime = time() # start time\n",
    "    testRDD2 = normTFIDF(rdd2, testDim, False) \n",
    "#>>>  # call an action on the RDD to trigger execution\n",
    "    testRDD2.count() # call an action on the RDD to trigger execution (in this case count() has been used as the action)\n",
    "#>>>  # end timer\n",
    "    endTime = time() # end time\n",
    "    resNoCache.append( endTime - startTime ) # calculate the time spent\n",
    "    \n",
    "#>>> meanTimeCaching = # calculate average times\n",
    "meanTimeCaching = sum(resCaching) / len(resCaching) # calculate average times\n",
    "#>>> meanTimeNoCache = # calculate average times\n",
    "meanTimeNoCache = sum(resNoCache) / len(resNoCache) # calculate average times\n",
    "\n",
    "print('Creating TF.IDF vectors, 3 trials - mean time with caching: ', meanTimeCaching, ', mean time without caching: ', meanTimeNoCache)\n",
    "#>>> # add your results and comments here \n",
    "# Results (average): Caching = 17.605319897333782s | No Caching = 23.110472758611042s\n",
    "# Comment: Caching, in the context of Spark, is a way to speed up operations / applications\n",
    "#          that access the same RDD multiple times. RDDs that are not cached have \n",
    "#          to be re-evaluated again each time an action is called. Therefore, the \n",
    "#          average time for the experiment above is expected to be quicker for caching than\n",
    "#          without caching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f-xzB6hrkeq5"
   },
   "source": [
    "## Task 4) Create LabeledPoints \n",
    "\n",
    "Determine whether the file is spam (i.e. the filename contains ’spmsg’) and replace the filename by a 1 (spam) or 0 (non-spam) accordingly. Use `RDD.map()` to create an RDD of LabeledPoint objects. See here [http://spark.apache.org/docs/2.1.0/mllib-linear-methods.html#logistic-regression](http://spark.apache.org/docs/2.1.0/mllib-linear-methods.html#logistic-regression) for an example, and here [http://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint](http://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint) for the `LabeledPoint` documentation. (10%)\n",
    "\n",
    "There is a handy function of Python strings called startswith: e.g. 'abc'.startswith('ab) will return true. The relevant Python syntax here is a conditional expression: **``<a> if <yourCondition> else <b>``**, i.e. 1 if the filename starts with 'spmsg' and otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wF9BDmnEkeq6",
    "outputId": "e2424977-ab97-4117-be29-85c8691f1813",
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [0.0,0.16290896085571283,0.6826175329317583,0.0,0.0,0.0,0.40170165983309447,0.32581792171142565,0.3132864631840631,0.3765953060935261])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# creates labelled points of vector size N out of an RDD with normalised (filename [(word,count), ...]) items\n",
    "def makeLabeledPoints(fn_vec_RDD): # RDD and N needed \n",
    "    # we determine the true class as encoded in the filename and represent as 1 (samp) or 0 (good)\n",
    "#>>>    cls_vec_RDD = fn_vec_RDD.map(lambda ... ) # use a conditional expression to get the class label (True or False)\n",
    "    cls_vec_RDD = fn_vec_RDD.map(lambda x: (1 if x[0].startswith('spmsg') == True else 0, x[1])) # use a conditional expression to get the class label (True or False)\n",
    "    # now we can create the LabeledPoint objects with (class,vector) arguments\n",
    "    lp_RDD = cls_vec_RDD.map(lambda cls_vec: LabeledPoint(cls_vec[0],cls_vec[1]) ) \n",
    "    return lp_RDD \n",
    "\n",
    "# for testing\n",
    "testLpRDD = makeLabeledPoints(rdd3) \n",
    "print(testLpRDD.take(1)) \n",
    "# should look similar to this: [LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.40968062880166006,0.0,0.0,0.0,0.9122290186048,0.0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vuLuz17zkeq8"
   },
   "source": [
    "## Task 5) Complete the preprocessing \n",
    "\n",
    "It will be useful to have a single function to do the preprocessing. So integrate everything here. (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "h9F8Kl3ukeq9",
    "outputId": "008e6ad7-3546-44e4-cde9-1710d0686150"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [0.0,0.16290896085571283,0.6826175329317583,0.0,0.0,0.0,0.40170165983309447,0.32581792171142565,0.3132864631840631,0.3765953060935261])]\n",
      "[PosixPath('lemm/part10'), PosixPath('lemm/part9'), PosixPath('lemm/part8'), PosixPath('lemm/part7'), PosixPath('lemm/part6'), PosixPath('lemm/part5'), PosixPath('lemm/part4'), PosixPath('lemm/part3'), PosixPath('lemm/part2'), PosixPath('lemm/part1')]\n",
      "len(rddList) 10\n",
      "[('file:/content/drive/My Drive/BigData/data/lingspam_public/lemm/part9/8-829msg1.txt', \"Subject: re : 8 . 810 , sum : french loan word , language evolution\\n\\njust see your summary on french loanword in english . i think baugh take the 10 , 0 word figure from jespersen 's _ growth and structure of the english language _ . it would be more like jespersen than baugh to actually try to count them , in fact . it 's worth look at jespersen for all sort of fact like these , in his chapter on the french influence in that book . - - suzanne kemmer\\n\")]\n",
      "[LabeledPoint(0.0, [0.04138897380669189,0.06905413571056576,0.09383464193692988,0.05512201563970116,0.07984722995624746,0.18013185637317497,0.0,0.20822625541036702,0.07069696035842367,0.13547187640721076,0.0,0.06707475330330137,0.0,0.13709891148926243,0.06970516550511324,0.13157111094298884,0.0,0.018695161508859446,0.06004395212439166,0.14080467630467874,0.052631737455157514,0.031066880462049386,0.0,0.0,0.0,0.08404971781482777,0.11493762390908602,0.07386494891693535,0.0,0.0,0.0,0.0,0.0,0.0,0.1199394317427922,0.0,0.17730764663842596,0.0,0.10528784819161091,0.0,0.0,0.3070050802808594,0.053543655934717986,0.1723678189276023,0.0,0.0,0.08942769316152013,0.12008790424878332,0.10885318879037179,0.07777804020231302,0.05172388098674139,0.07471311868383705,0.030641126463577917,0.32355902487976923,0.0,0.0563041510483258,0.053543655934717986,0.0,0.0,0.09607437260966498,0.0,0.0,0.0,0.0,0.27487269420506455,0.052819036192073655,0.0,0.0,0.0,0.23266189576249866,0.023830140138709842,0.35771077264608053,0.0,0.08286496285267891,0.1535025401404297,0.07984722995624746,0.2061545206537984,0.0,0.0,0.11370611901324568,0.06530066111419289,0.0,0.0,0.0,0.11633094788124933,0.0,0.03603818643395556,0.07881000919268093,0.0,0.08420783056445955,0.0,0.130327077944359,0.12120930152160075,0.06578555547149442,0.0,0.05083248990813509,0.0,0.0,0.0,0.0])]\n",
      "[LabeledPoint(0.0, [0.039040846770928436,0.022674391863931637,0.05382002095379722,0.04150467469020853,0.05635234101139964,0.07915689212662691,0.046942949727828004,0.0287849031640231,0.03732873224682722,0.06169339060751134,0.0,0.3386496584233161,0.051662386799475636,0.05238069252089395,0.01928444193006618,0.08550557953919574,0.036354303516330747,0.019223852681304707,0.07825827311556759,0.3820547208945783,0.07382113653790466,0.04674636550685318,0.026385630708875635,0.01900984469827045,0.014343913662492585,0.2747689642505897,0.35728057051386114,0.03455987510596566,0.03972400848867156,0.08507392682477877,0.09385206654244857,0.019495200059093134,0.06211037183650699,0.11093364143285014,0.04129569637200316,0.017143434484527917,0.14651608662963908,0.06289599070490164,0.10479548039220947,0.010594392412826819,0.11334341460224598,0.20525220067163827,0.07984534968743723,0.05251668659182594,0.009994237667291003,0.04007975896016933,0.10719440894217452,0.06263713015836268,0.16272355677620967,0.0582509418324443,0.07119877145420032,0.041655958381150916,0.019809573544071663,0.09252359064234067,0.03877380701163336,0.05052704169569673,0.11502588164224686,0.1493149289873089,0.0482765902530004,0.09054485361876083,0.27801449661245586,0.02026044039029053,0.012699723766866476,0.05150762156260405,0.13797376305844428,0.04515418034146933,0.057351790020233655,0.017143434484527917,0.014077859344051565,0.06487721675568311,0.035523002917341055,0.06116921813829097,0.014611503052370299,0.022229519150869643,0.0,0.12410413598204406,0.03397955702473912,0.027368337549214476,0.03637136019169334,0.07923831530917867,0.025351121998979348,0.16177813892965795,0.023820979764927672,0.0,0.10863641226342821,0.01512668292543555,0.05856741249628033,0.13488700804095688,0.018212038708586394,0.04067992482656352,0.06336799353788941,0.0,0.08374456966190376,0.065316896070828,0.0,0.04841316230630353,0.07087214934518402,0.0425958118571764,0.055235546680043844,0.02150252839134035])]\n"
     ]
    }
   ],
   "source": [
    "# now we can apply the preprocessing chain to the data loaded in task 1 \n",
    "# N is for controlling the vector size\n",
    "def preprocess(rawRDD,N):\n",
    "    \"\"\" take a (filename,text) RDD and transform into LabelledPoint objects \n",
    "        with class labels and a TF.IDF vector with N dimensions. \n",
    "    \"\"\"\n",
    "    tokenRDD = prepareTokenRDD(rawRDD) # task 2\n",
    "    tfIdfRDD = normTFIDF(tokenRDD,N) # task 3\n",
    "    lpRDD = makeLabeledPoints(tfIdfRDD) # task 4\n",
    "    return lpRDD # return RDD with LabeledPoints\n",
    "\n",
    "# and with this we can start the whole process from a directory, N is again the vector size\n",
    "def loadAndPreprocess(directory,N):\n",
    "    \"\"\" load lingspam data from a directory and create a training and test set of preprocessed data \"\"\"\n",
    "#>>>    trainRDD_testRDD = ... # read from the directory using the function created in task 1\n",
    "    trainRDD_testRDD = makeTestTrainRDDs(directory) # read from the directory using the function created in task 1\n",
    "#>>>     # unpack the returned tuple\n",
    "    trainRDD, testRDD = trainRDD_testRDD # unpack the tuple\n",
    "    return (preprocess(trainRDD,N),preprocess(testRDD,N)) # apply the preprocessing funcion defined above\n",
    "\n",
    "trainLpRDD = preprocess(trainRDD,testDim) # prepare the training data\n",
    "print(testLpRDD.take(1)) # should look similar to previous cell's output\n",
    "\n",
    "train_test_LpRDD = loadAndPreprocess('lemm',100) # let's re-run with another vector size\n",
    "(trainLpRDD,testLpRDD) = train_test_LpRDD \n",
    "print(testLpRDD.take(1))\n",
    "print(trainLpRDD.take(1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdwyXqu4keq_"
   },
   "source": [
    "## Task 6) Train some classifiers \n",
    "\n",
    "Use the `LabeledPoint` objects to train a classifier, specifically the *LogisticRegression*, *Naive Bayes*, and *Support Vector Machine*. Calculate the accuracy of the model on the training set (again, follow this example [http://spark.apache.org/docs/2.1.0/ml-classification-regression.html#logistic-regression](http://spark.apache.org/docs/2.0.0/ml-classification-regression.html#logistic-regression) and here is the documentation for the classifiers [LogisticRegressionWithLBFGS](http://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionWithLBFGS), [NaiveBayes](http://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.classification.NaiveBayes), [SVMWithSGD](http://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.classification.SVMWithSGD).  (10%) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "Ljcgl0mLkeq_",
    "outputId": "c6c59046-becf-404d-8825-311047d2d799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8337173579109063"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.classification import (NaiveBayes, LogisticRegressionWithLBFGS, SVMWithSGD) \n",
    "import numpy\n",
    "\n",
    "# train the model with an (f,[(w,c), ...]) RDD. This is practical as we can reuse the function for TF.IDF\n",
    "def trainModel(lpRDD):\n",
    "    \"\"\" Train 3 classifier models on the given RDD with LabeledPoint objects. A list of trained model is returned. \"\"\"\n",
    "    lpRDD.persist(StorageLevel.MEMORY_ONLY) # not really needed as the Spark implementations ensure caching themselves. \n",
    "                    # Other implementations might not, however. \n",
    "    # Train a classifier model.\n",
    "    print('Starting to train the model') # give some immediate feedback\n",
    "    model1 = LogisticRegressionWithLBFGS.train(lpRDD) # this is the best model\n",
    "    print('Trained LR (model1)')\n",
    "    #print('type(model1)')\n",
    "    model2 = NaiveBayes.train(lpRDD) # doesn't work well\n",
    "    print('Trained NB (model2)')\n",
    "    #print(type(model2))\n",
    "    model3 = SVMWithSGD.train(lpRDD) # or this ...\n",
    "    print('Trained SVM (model3)')\n",
    "    return [model1,model2,model3]\n",
    "\n",
    "def testModel(model, lpRDD):\n",
    "    \"\"\" Tests the classification accuracy of the given model on the given RDD with LabeledPoint objects. \"\"\"\n",
    "    lpRDD.persist(StorageLevel.MEMORY_ONLY)\n",
    "    # Make prediction and evaluate training set accuracy.\n",
    "    # Get the prediction and the ground truth label\n",
    "    predictionAndLabel = lpRDD.map(lambda p: (model.predict(p.features), p.label)) # get the prediction and ground truth (label) for each item.\n",
    "    correct = predictionAndLabel.filter(lambda xv: xv[0] == xv[1]).count() # count the correct predictions \n",
    "#>>>    accuracy = ... # and calculate the accuracy\n",
    "    accuracy = correct / lpRDD.count() # and calculate the accuracy\n",
    "    print('Accuracy {:.1%} (data items: {}, correct: {})'.format(accuracy,lpRDD.count(), correct)) # report to console\n",
    "    return accuracy # and return the value  \n",
    "\n",
    "models = trainModel(trainLpRDD) # just for testing\n",
    "testModel(models[2], trainLpRDD) # just for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8TeHFXcTkerC"
   },
   "source": [
    "## Task 7) Automate training and testing\n",
    "\n",
    "We automate now the whole process from reading the files, through preprocessing, and training up to evaluating the models. In the end we have a single function that takes all the parameters we are interested in and produces trained models and an evaluation. (5%) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "Kj_Xh5oekerD",
    "outputId": "9ecd7507-728e-4e36-d4ad-ad43e11b96d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading and preprocessing\n",
      "[PosixPath('lemm/part10'), PosixPath('lemm/part9'), PosixPath('lemm/part8'), PosixPath('lemm/part7'), PosixPath('lemm/part6'), PosixPath('lemm/part5'), PosixPath('lemm/part4'), PosixPath('lemm/part3'), PosixPath('lemm/part2'), PosixPath('lemm/part1')]\n",
      "len(rddList) 10\n",
      "[('file:/content/drive/My Drive/BigData/data/lingspam_public/lemm/part9/8-829msg1.txt', \"Subject: re : 8 . 810 , sum : french loan word , language evolution\\n\\njust see your summary on french loanword in english . i think baugh take the 10 , 0 word figure from jespersen 's _ growth and structure of the english language _ . it would be more like jespersen than baugh to actually try to count them , in fact . it 's worth look at jespersen for all sort of fact like these , in his chapter on the french influence in that book . - - suzanne kemmer\\n\")]\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
      "Testing\n",
      "Accuracy 97.2% (data items: 289, correct: 281)\n",
      "Training\n",
      "Accuracy 94.4% (data items: 2604, correct: 2459)\n",
      "Testing\n",
      "Accuracy 92.7% (data items: 289, correct: 268)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1.0, 0.9443164362519201, 0.8337173579109063],\n",
       " [0.972318339100346, 0.9273356401384083, 0.8339100346020761]]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this function combines tasks f) and g)\n",
    "# this method should take RDDs with (f,[(w,c), ...])\n",
    "def trainTestModel(trainRDD,testRDD):\n",
    "    \"\"\" Trains 3 models and tests them on training and test data. Returns a matrix the training and testing (rows) accuracy values for all models (columns). \"\"\"\n",
    "#>>> models = ... # train models on the training set\n",
    "    models = trainModel(trainRDD) # train models on the training set\n",
    "    results = [[],[]] # matrix for 2 modes (training/test) vs n models (currently 3)\n",
    "    for mdl in models:\n",
    "        print('Training')\n",
    "#>>>     results[0].append(...) # test the model on the training set\n",
    "        results[0].append(testModel(mdl, trainRDD)) # test the model on the training set\n",
    "        print('Testing')\n",
    "#>>>     results[1].append(...) # test the model on the test set\n",
    "        results[1].append(testModel(mdl, testRDD)) # test the model on the test set\n",
    "    return results\n",
    "\n",
    "def trainTestFolder(folder,N): \n",
    "    \"\"\" Reads data from a folder, preproceses the data, and trains and evaluates models on it. \"\"\"\n",
    "    print('Start loading and preprocessing') \n",
    "    train_test_LpRDD = loadAndPreprocess(folder,N) # create the RDDs\n",
    "    print('Finished loading and preprocessing')\n",
    "    (trainLpRDD,testLpRDD) = train_test_LpRDD # unpack the RDDs \n",
    "    return trainTestModel(trainLpRDD,testLpRDD) # train and test\n",
    "\n",
    "trainTestFolder('lemm',1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pzcNex1NkerF"
   },
   "source": [
    "## Task 8) Run experiments \n",
    "\n",
    "We have now a single function that allows us to vary the vector size easily. Test vector sizes 3, 30, 300, 3000, 30000 and examine the effect on the classification accuracy in Experiment 1.\n",
    "\n",
    "Use the function from Task 7) to test different data types. The dataset has raw text in folder `bare`, lemmatised text in  `lemm` (similar to stemming, reduces to basic word forms), `stop` (with stopwords removed), and `lemm_stop` (lemmatised and stopwords removed). Test how the classification accuracy differs for these four data types in Experiment 2. Collect the results in a data structure that can be saved for later saving and analyis.\n",
    "\n",
    "Comment on the results in a few sentences, considering the differences in performance between the different conditions as well as train an test values. 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3770
    },
    "colab_type": "code",
    "id": "HmW_CpetkerG",
    "outputId": "6c987e20-9c86-4252-c4ab-2052c274fb4a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXPERIMENT 1: Testing different vector sizes\n",
      "==========================================================================================================================================================================================\n",
      "N = 3\n",
      "Start loading and preprocessing\n",
      "[PosixPath('bare/part10'), PosixPath('bare/part9'), PosixPath('bare/part8'), PosixPath('bare/part7'), PosixPath('bare/part6'), PosixPath('bare/part5'), PosixPath('bare/part4'), PosixPath('bare/part3'), PosixPath('bare/part2'), PosixPath('bare/part1')]\n",
      "len(rddList) 10\n",
      "[('file:/content/drive/My Drive/BigData/data/lingspam_public/bare/part9/8-817msg1.txt', 'Subject: disc : grammar in uk schools\\n\\ni was disturbed to find some traditional fallacies in geoffrey sampson \\'s discussion of the teaching of grammar in schools . though i no longer have a copy of prof . cameron \\'s original post , i do recall the essentials of it , and found that mr . sampson had passed over the valid point it was making in favour of a prescriptivist , \" back to basics \" defense of traditional grammatical education . prof . cameron is perfectly correct in ridiculing the inflexible , rote and prescriptive approach to grammar which is conventionally inflicted on students throughout the english speaking world . the issue of being able to use standard english ( or perhaps _ a _ standard english ) correctly is entirely separate from the reliance on traditional \" rules \" which are frequently unhelpful , and often grossly inaccurate . the rule regarding finishing sentences with prepositions , as one glaring example , is a total misunderstanding of both the history of english , and an unhelpful preoccupation for effective communication . > strikes me as akin to suggesting that teachers of > french should forget about teaching the past participle of \" vivre \" in > favour of getting their pupils to develop considered opinions about > the theories of derrida . though mr . sampson has used an interesting rhetorical image here , it is in fact a false analogy . teaching students to get a feel for the function of grammar and language is a far cry from teaching them gb theory or hpsg . an understanding of how sentences , clauses , verb tenses , adverbs , etc . actually function on a basic level is a very reasonable educational goal , and far more worthy then just creating a bunch of \" do n\\'t \" \\'s and \" never \" \\'s and calling that grammatical education . > beyond that , though , teaching orthography and grammar at school level > has a much broader educational value . one of the lessons we all have > to learn is that nothing big and worthwhile is ever achieved in this > life without careful attention to endless tedious and often arbitrary > details . at the risk of making a gross national stereotype , i feel compelled to quote george bernard shaw : \" the british believe that they are moral when they are merely uncomfortable . \" this notion that education ( or work , for that matter ) must be unpleasant to produce results is a puritanical relic . in my personal experience , the very successful people tend to be precisely the ones who know how to delegate , slough off or avoid wasting time with \" tedious and arbitrary details \" . ( please read the preceding paragraph with the tongue planted in the general vicinity of the cheek . ) in a spirit of greater seriousness though , i would like to second prof . cameron \\'s call to educators to abandon prescriptive , rule-based approaches to grammar , and embrace a more general approach based on a comprehension of more fluid and meaningful principles . i believe that the result would be students with a better grasp of the form and function of language rather than a shallow and inflexible mastery of facile rules . - - - - marc hamann\\n')]\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "==========================================================================================================================================================================================\n",
      "N = 30\n",
      "Start loading and preprocessing\n",
      "[PosixPath('bare/part10'), PosixPath('bare/part9'), PosixPath('bare/part8'), PosixPath('bare/part7'), PosixPath('bare/part6'), PosixPath('bare/part5'), PosixPath('bare/part4'), PosixPath('bare/part3'), PosixPath('bare/part2'), PosixPath('bare/part1')]\n",
      "len(rddList) 10\n",
      "[('file:/content/drive/My Drive/BigData/data/lingspam_public/bare/part9/8-817msg1.txt', 'Subject: disc : grammar in uk schools\\n\\ni was disturbed to find some traditional fallacies in geoffrey sampson \\'s discussion of the teaching of grammar in schools . though i no longer have a copy of prof . cameron \\'s original post , i do recall the essentials of it , and found that mr . sampson had passed over the valid point it was making in favour of a prescriptivist , \" back to basics \" defense of traditional grammatical education . prof . cameron is perfectly correct in ridiculing the inflexible , rote and prescriptive approach to grammar which is conventionally inflicted on students throughout the english speaking world . the issue of being able to use standard english ( or perhaps _ a _ standard english ) correctly is entirely separate from the reliance on traditional \" rules \" which are frequently unhelpful , and often grossly inaccurate . the rule regarding finishing sentences with prepositions , as one glaring example , is a total misunderstanding of both the history of english , and an unhelpful preoccupation for effective communication . > strikes me as akin to suggesting that teachers of > french should forget about teaching the past participle of \" vivre \" in > favour of getting their pupils to develop considered opinions about > the theories of derrida . though mr . sampson has used an interesting rhetorical image here , it is in fact a false analogy . teaching students to get a feel for the function of grammar and language is a far cry from teaching them gb theory or hpsg . an understanding of how sentences , clauses , verb tenses , adverbs , etc . actually function on a basic level is a very reasonable educational goal , and far more worthy then just creating a bunch of \" do n\\'t \" \\'s and \" never \" \\'s and calling that grammatical education . > beyond that , though , teaching orthography and grammar at school level > has a much broader educational value . one of the lessons we all have > to learn is that nothing big and worthwhile is ever achieved in this > life without careful attention to endless tedious and often arbitrary > details . at the risk of making a gross national stereotype , i feel compelled to quote george bernard shaw : \" the british believe that they are moral when they are merely uncomfortable . \" this notion that education ( or work , for that matter ) must be unpleasant to produce results is a puritanical relic . in my personal experience , the very successful people tend to be precisely the ones who know how to delegate , slough off or avoid wasting time with \" tedious and arbitrary details \" . ( please read the preceding paragraph with the tongue planted in the general vicinity of the cheek . ) in a spirit of greater seriousness though , i would like to second prof . cameron \\'s call to educators to abandon prescriptive , rule-based approaches to grammar , and embrace a more general approach based on a comprehension of more fluid and meaningful principles . i believe that the result would be students with a better grasp of the form and function of language rather than a shallow and inflexible mastery of facile rules . - - - - marc hamann\\n')]\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 85.8% (data items: 2604, correct: 2233)\n",
      "Testing\n",
      "Accuracy 84.8% (data items: 289, correct: 245)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "==========================================================================================================================================================================================\n",
      "N = 300\n",
      "Start loading and preprocessing\n",
      "[PosixPath('bare/part10'), PosixPath('bare/part9'), PosixPath('bare/part8'), PosixPath('bare/part7'), PosixPath('bare/part6'), PosixPath('bare/part5'), PosixPath('bare/part4'), PosixPath('bare/part3'), PosixPath('bare/part2'), PosixPath('bare/part1')]\n",
      "len(rddList) 10\n",
      "[('file:/content/drive/My Drive/BigData/data/lingspam_public/bare/part9/8-817msg1.txt', 'Subject: disc : grammar in uk schools\\n\\ni was disturbed to find some traditional fallacies in geoffrey sampson \\'s discussion of the teaching of grammar in schools . though i no longer have a copy of prof . cameron \\'s original post , i do recall the essentials of it , and found that mr . sampson had passed over the valid point it was making in favour of a prescriptivist , \" back to basics \" defense of traditional grammatical education . prof . cameron is perfectly correct in ridiculing the inflexible , rote and prescriptive approach to grammar which is conventionally inflicted on students throughout the english speaking world . the issue of being able to use standard english ( or perhaps _ a _ standard english ) correctly is entirely separate from the reliance on traditional \" rules \" which are frequently unhelpful , and often grossly inaccurate . the rule regarding finishing sentences with prepositions , as one glaring example , is a total misunderstanding of both the history of english , and an unhelpful preoccupation for effective communication . > strikes me as akin to suggesting that teachers of > french should forget about teaching the past participle of \" vivre \" in > favour of getting their pupils to develop considered opinions about > the theories of derrida . though mr . sampson has used an interesting rhetorical image here , it is in fact a false analogy . teaching students to get a feel for the function of grammar and language is a far cry from teaching them gb theory or hpsg . an understanding of how sentences , clauses , verb tenses , adverbs , etc . actually function on a basic level is a very reasonable educational goal , and far more worthy then just creating a bunch of \" do n\\'t \" \\'s and \" never \" \\'s and calling that grammatical education . > beyond that , though , teaching orthography and grammar at school level > has a much broader educational value . one of the lessons we all have > to learn is that nothing big and worthwhile is ever achieved in this > life without careful attention to endless tedious and often arbitrary > details . at the risk of making a gross national stereotype , i feel compelled to quote george bernard shaw : \" the british believe that they are moral when they are merely uncomfortable . \" this notion that education ( or work , for that matter ) must be unpleasant to produce results is a puritanical relic . in my personal experience , the very successful people tend to be precisely the ones who know how to delegate , slough off or avoid wasting time with \" tedious and arbitrary details \" . ( please read the preceding paragraph with the tongue planted in the general vicinity of the cheek . ) in a spirit of greater seriousness though , i would like to second prof . cameron \\'s call to educators to abandon prescriptive , rule-based approaches to grammar , and embrace a more general approach based on a comprehension of more fluid and meaningful principles . i believe that the result would be students with a better grasp of the form and function of language rather than a shallow and inflexible mastery of facile rules . - - - - marc hamann\\n')]\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
      "Testing\n",
      "Accuracy 93.1% (data items: 289, correct: 269)\n",
      "Training\n",
      "Accuracy 86.1% (data items: 2604, correct: 2243)\n",
      "Testing\n",
      "Accuracy 84.8% (data items: 289, correct: 245)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "==========================================================================================================================================================================================\n",
      "N = 3000\n",
      "Start loading and preprocessing\n",
      "[PosixPath('bare/part10'), PosixPath('bare/part9'), PosixPath('bare/part8'), PosixPath('bare/part7'), PosixPath('bare/part6'), PosixPath('bare/part5'), PosixPath('bare/part4'), PosixPath('bare/part3'), PosixPath('bare/part2'), PosixPath('bare/part1')]\n",
      "len(rddList) 10\n",
      "[('file:/content/drive/My Drive/BigData/data/lingspam_public/bare/part9/8-817msg1.txt', 'Subject: disc : grammar in uk schools\\n\\ni was disturbed to find some traditional fallacies in geoffrey sampson \\'s discussion of the teaching of grammar in schools . though i no longer have a copy of prof . cameron \\'s original post , i do recall the essentials of it , and found that mr . sampson had passed over the valid point it was making in favour of a prescriptivist , \" back to basics \" defense of traditional grammatical education . prof . cameron is perfectly correct in ridiculing the inflexible , rote and prescriptive approach to grammar which is conventionally inflicted on students throughout the english speaking world . the issue of being able to use standard english ( or perhaps _ a _ standard english ) correctly is entirely separate from the reliance on traditional \" rules \" which are frequently unhelpful , and often grossly inaccurate . the rule regarding finishing sentences with prepositions , as one glaring example , is a total misunderstanding of both the history of english , and an unhelpful preoccupation for effective communication . > strikes me as akin to suggesting that teachers of > french should forget about teaching the past participle of \" vivre \" in > favour of getting their pupils to develop considered opinions about > the theories of derrida . though mr . sampson has used an interesting rhetorical image here , it is in fact a false analogy . teaching students to get a feel for the function of grammar and language is a far cry from teaching them gb theory or hpsg . an understanding of how sentences , clauses , verb tenses , adverbs , etc . actually function on a basic level is a very reasonable educational goal , and far more worthy then just creating a bunch of \" do n\\'t \" \\'s and \" never \" \\'s and calling that grammatical education . > beyond that , though , teaching orthography and grammar at school level > has a much broader educational value . one of the lessons we all have > to learn is that nothing big and worthwhile is ever achieved in this > life without careful attention to endless tedious and often arbitrary > details . at the risk of making a gross national stereotype , i feel compelled to quote george bernard shaw : \" the british believe that they are moral when they are merely uncomfortable . \" this notion that education ( or work , for that matter ) must be unpleasant to produce results is a puritanical relic . in my personal experience , the very successful people tend to be precisely the ones who know how to delegate , slough off or avoid wasting time with \" tedious and arbitrary details \" . ( please read the preceding paragraph with the tongue planted in the general vicinity of the cheek . ) in a spirit of greater seriousness though , i would like to second prof . cameron \\'s call to educators to abandon prescriptive , rule-based approaches to grammar , and embrace a more general approach based on a comprehension of more fluid and meaningful principles . i believe that the result would be students with a better grasp of the form and function of language rather than a shallow and inflexible mastery of facile rules . - - - - marc hamann\\n')]\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
      "Testing\n",
      "Accuracy 96.9% (data items: 289, correct: 280)\n",
      "Training\n",
      "Accuracy 97.4% (data items: 2604, correct: 2536)\n",
      "Testing\n",
      "Accuracy 96.2% (data items: 289, correct: 278)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "==========================================================================================================================================================================================\n",
      "N = 30000\n",
      "Start loading and preprocessing\n",
      "[PosixPath('bare/part10'), PosixPath('bare/part9'), PosixPath('bare/part8'), PosixPath('bare/part7'), PosixPath('bare/part6'), PosixPath('bare/part5'), PosixPath('bare/part4'), PosixPath('bare/part3'), PosixPath('bare/part2'), PosixPath('bare/part1')]\n",
      "len(rddList) 10\n",
      "[('file:/content/drive/My Drive/BigData/data/lingspam_public/bare/part9/8-817msg1.txt', 'Subject: disc : grammar in uk schools\\n\\ni was disturbed to find some traditional fallacies in geoffrey sampson \\'s discussion of the teaching of grammar in schools . though i no longer have a copy of prof . cameron \\'s original post , i do recall the essentials of it , and found that mr . sampson had passed over the valid point it was making in favour of a prescriptivist , \" back to basics \" defense of traditional grammatical education . prof . cameron is perfectly correct in ridiculing the inflexible , rote and prescriptive approach to grammar which is conventionally inflicted on students throughout the english speaking world . the issue of being able to use standard english ( or perhaps _ a _ standard english ) correctly is entirely separate from the reliance on traditional \" rules \" which are frequently unhelpful , and often grossly inaccurate . the rule regarding finishing sentences with prepositions , as one glaring example , is a total misunderstanding of both the history of english , and an unhelpful preoccupation for effective communication . > strikes me as akin to suggesting that teachers of > french should forget about teaching the past participle of \" vivre \" in > favour of getting their pupils to develop considered opinions about > the theories of derrida . though mr . sampson has used an interesting rhetorical image here , it is in fact a false analogy . teaching students to get a feel for the function of grammar and language is a far cry from teaching them gb theory or hpsg . an understanding of how sentences , clauses , verb tenses , adverbs , etc . actually function on a basic level is a very reasonable educational goal , and far more worthy then just creating a bunch of \" do n\\'t \" \\'s and \" never \" \\'s and calling that grammatical education . > beyond that , though , teaching orthography and grammar at school level > has a much broader educational value . one of the lessons we all have > to learn is that nothing big and worthwhile is ever achieved in this > life without careful attention to endless tedious and often arbitrary > details . at the risk of making a gross national stereotype , i feel compelled to quote george bernard shaw : \" the british believe that they are moral when they are merely uncomfortable . \" this notion that education ( or work , for that matter ) must be unpleasant to produce results is a puritanical relic . in my personal experience , the very successful people tend to be precisely the ones who know how to delegate , slough off or avoid wasting time with \" tedious and arbitrary details \" . ( please read the preceding paragraph with the tongue planted in the general vicinity of the cheek . ) in a spirit of greater seriousness though , i would like to second prof . cameron \\'s call to educators to abandon prescriptive , rule-based approaches to grammar , and embrace a more general approach based on a comprehension of more fluid and meaningful principles . i believe that the result would be students with a better grasp of the form and function of language rather than a shallow and inflexible mastery of facile rules . - - - - marc hamann\\n')]\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
      "Testing\n",
      "Accuracy 97.2% (data items: 289, correct: 281)\n",
      "Training\n",
      "Accuracy 86.3% (data items: 2604, correct: 2246)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2172)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "EXPERIMENT 2: Testing different data types\n",
      "==========================================================================================================================================================================================\n",
      "Path = bare\n",
      "Start loading and preprocessing\n",
      "[PosixPath('bare/part10'), PosixPath('bare/part9'), PosixPath('bare/part8'), PosixPath('bare/part7'), PosixPath('bare/part6'), PosixPath('bare/part5'), PosixPath('bare/part4'), PosixPath('bare/part3'), PosixPath('bare/part2'), PosixPath('bare/part1')]\n",
      "len(rddList) 10\n",
      "[('file:/content/drive/My Drive/BigData/data/lingspam_public/bare/part9/8-817msg1.txt', 'Subject: disc : grammar in uk schools\\n\\ni was disturbed to find some traditional fallacies in geoffrey sampson \\'s discussion of the teaching of grammar in schools . though i no longer have a copy of prof . cameron \\'s original post , i do recall the essentials of it , and found that mr . sampson had passed over the valid point it was making in favour of a prescriptivist , \" back to basics \" defense of traditional grammatical education . prof . cameron is perfectly correct in ridiculing the inflexible , rote and prescriptive approach to grammar which is conventionally inflicted on students throughout the english speaking world . the issue of being able to use standard english ( or perhaps _ a _ standard english ) correctly is entirely separate from the reliance on traditional \" rules \" which are frequently unhelpful , and often grossly inaccurate . the rule regarding finishing sentences with prepositions , as one glaring example , is a total misunderstanding of both the history of english , and an unhelpful preoccupation for effective communication . > strikes me as akin to suggesting that teachers of > french should forget about teaching the past participle of \" vivre \" in > favour of getting their pupils to develop considered opinions about > the theories of derrida . though mr . sampson has used an interesting rhetorical image here , it is in fact a false analogy . teaching students to get a feel for the function of grammar and language is a far cry from teaching them gb theory or hpsg . an understanding of how sentences , clauses , verb tenses , adverbs , etc . actually function on a basic level is a very reasonable educational goal , and far more worthy then just creating a bunch of \" do n\\'t \" \\'s and \" never \" \\'s and calling that grammatical education . > beyond that , though , teaching orthography and grammar at school level > has a much broader educational value . one of the lessons we all have > to learn is that nothing big and worthwhile is ever achieved in this > life without careful attention to endless tedious and often arbitrary > details . at the risk of making a gross national stereotype , i feel compelled to quote george bernard shaw : \" the british believe that they are moral when they are merely uncomfortable . \" this notion that education ( or work , for that matter ) must be unpleasant to produce results is a puritanical relic . in my personal experience , the very successful people tend to be precisely the ones who know how to delegate , slough off or avoid wasting time with \" tedious and arbitrary details \" . ( please read the preceding paragraph with the tongue planted in the general vicinity of the cheek . ) in a spirit of greater seriousness though , i would like to second prof . cameron \\'s call to educators to abandon prescriptive , rule-based approaches to grammar , and embrace a more general approach based on a comprehension of more fluid and meaningful principles . i believe that the result would be students with a better grasp of the form and function of language rather than a shallow and inflexible mastery of facile rules . - - - - marc hamann\\n')]\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
      "Testing\n",
      "Accuracy 96.9% (data items: 289, correct: 280)\n",
      "Training\n",
      "Accuracy 97.4% (data items: 2604, correct: 2536)\n",
      "Testing\n",
      "Accuracy 96.2% (data items: 289, correct: 278)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "==========================================================================================================================================================================================\n",
      "Path = stop\n",
      "Start loading and preprocessing\n",
      "[PosixPath('stop/part10'), PosixPath('stop/part9'), PosixPath('stop/part8'), PosixPath('stop/part7'), PosixPath('stop/part6'), PosixPath('stop/part5'), PosixPath('stop/part4'), PosixPath('stop/part3'), PosixPath('stop/part2'), PosixPath('stop/part1')]\n",
      "len(rddList) 10\n",
      "[('file:/content/drive/My Drive/BigData/data/lingspam_public/stop/part9/8-817msg1.txt', 'Subject: disc : grammar uk schools\\n\\nwas disturbed traditional fallacies geoffrey sampson \\'s discussion teaching grammar schools . though longer copy prof . cameron \\'s original post , recall essentials , found mr . sampson had passed over valid point was making favour prescriptivist , \" back basics \" defense traditional grammatical education . prof . cameron is perfectly correct ridiculing inflexible , rote prescriptive approach grammar is conventionally inflicted students throughout english speaking world . issue being able standard english ( perhaps _ _ standard english ) correctly is entirely separate reliance traditional \" rules \" are frequently unhelpful , often grossly inaccurate . rule regarding finishing sentences prepositions , one glaring example , is total misunderstanding both history english , unhelpful preoccupation effective communication . > strikes akin suggesting teachers > french forget teaching past participle \" vivre \" > favour getting pupils develop considered opinions > theories derrida . though mr . sampson has used interesting rhetorical image here , is fact false analogy . teaching students feel function grammar language is far cry teaching gb theory hpsg . understanding sentences , clauses , verb tenses , adverbs , etc . actually function basic level is reasonable educational goal , far worthy creating bunch \" n\\'t \" \\'s \" never \" \\'s calling grammatical education . > beyond , though , teaching orthography grammar school level > has much broader educational value . one lessons > learn is nothing big worthwhile is ever achieved > life without careful attention endless tedious often arbitrary > details . risk making gross national stereotype , feel compelled quote george bernard shaw : \" british believe are moral are merely uncomfortable . \" notion education ( work , matter ) must unpleasant produce results is puritanical relic . personal experience , successful tend precisely ones delegate , slough off avoid wasting \" tedious arbitrary details \" . ( please read preceding paragraph tongue planted general vicinity cheek . ) spirit greater seriousness though , second prof . cameron \\'s call educators abandon prescriptive , rule-based approaches grammar , embrace general approach based comprehension fluid meaningful principles . believe result students better grasp form function language rather shallow inflexible mastery facile rules . - - - - marc hamann\\n')]\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
      "Testing\n",
      "Accuracy 97.6% (data items: 289, correct: 282)\n",
      "Training\n",
      "Accuracy 96.7% (data items: 2604, correct: 2518)\n",
      "Testing\n",
      "Accuracy 95.2% (data items: 289, correct: 275)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "==========================================================================================================================================================================================\n",
      "Path = lemm\n",
      "Start loading and preprocessing\n",
      "[PosixPath('lemm/part10'), PosixPath('lemm/part9'), PosixPath('lemm/part8'), PosixPath('lemm/part7'), PosixPath('lemm/part6'), PosixPath('lemm/part5'), PosixPath('lemm/part4'), PosixPath('lemm/part3'), PosixPath('lemm/part2'), PosixPath('lemm/part1')]\n",
      "len(rddList) 10\n",
      "[('file:/content/drive/My Drive/BigData/data/lingspam_public/lemm/part9/8-829msg1.txt', \"Subject: re : 8 . 810 , sum : french loan word , language evolution\\n\\njust see your summary on french loanword in english . i think baugh take the 10 , 0 word figure from jespersen 's _ growth and structure of the english language _ . it would be more like jespersen than baugh to actually try to count them , in fact . it 's worth look at jespersen for all sort of fact like these , in his chapter on the french influence in that book . - - suzanne kemmer\\n\")]\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
      "Testing\n",
      "Accuracy 97.6% (data items: 289, correct: 282)\n",
      "Training\n",
      "Accuracy 97.4% (data items: 2604, correct: 2535)\n",
      "Testing\n",
      "Accuracy 95.5% (data items: 289, correct: 276)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "==========================================================================================================================================================================================\n",
      "Path = lemm_stop\n",
      "Start loading and preprocessing\n",
      "[PosixPath('lemm_stop/part10'), PosixPath('lemm_stop/part9'), PosixPath('lemm_stop/part8'), PosixPath('lemm_stop/part7'), PosixPath('lemm_stop/part6'), PosixPath('lemm_stop/part5'), PosixPath('lemm_stop/part4'), PosixPath('lemm_stop/part3'), PosixPath('lemm_stop/part2'), PosixPath('lemm_stop/part1')]\n",
      "len(rddList) 10\n",
      "[('file:/content/drive/My Drive/BigData/data/lingspam_public/lemm_stop/part9/8-859msg1.txt', 'Subject: re : 8 . 836 , disc : punctuation\\n\\nseveral posting recently decry apostrophe- english plural form , e . g . \" dyslexic \\'s \" , \" sonata \\'s \" . grounds hold bid english ? everyone else , learn school apostrophe english plural except few exceptional case numeral letter ( \" 1990 \\'s \" , \" three \\'s \" ) . usage seem fashion recently , apostroph practice widely sort plural form , seem long history least foreign unusual word - \\' ve notice apostrophise plural facsimile several seventeenth century book recently . exactly grounds condemn usage ? john phillip\\n')]\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
      "Testing\n",
      "Accuracy 96.9% (data items: 289, correct: 280)\n",
      "Training\n",
      "Accuracy 96.9% (data items: 2604, correct: 2523)\n",
      "Testing\n",
      "Accuracy 94.8% (data items: 289, correct: 274)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "folder = 'bare'\n",
    "N = numpy.array([3,30,300,3000,30000]) \n",
    "print('\\nEXPERIMENT 1: Testing different vector sizes')\n",
    "results = []\n",
    "for n in N:\n",
    "    print('==========================================================================================================================================================================================')\n",
    "    print('N = {}'.format(n))\n",
    "    result = {'n':n, 't':folder}\n",
    "    result['acc'] = trainTestFolder(folder,n)\n",
    "    results.append(result)\n",
    "\n",
    "    \n",
    "n = 3000\n",
    "typeFolders = ['bare','stop','lemm','lemm_stop']\n",
    "print('EXPERIMENT 2: Testing different data types')\n",
    "results2 = []\n",
    "for folder in typeFolders:\n",
    "    print('==========================================================================================================================================================================================')\n",
    "    print('Path = {}'.format(folder))\n",
    "    result = {'n':n, 't':folder}\n",
    "    result['acc'] = trainTestFolder(folder,n)\n",
    "    results2.append(result)\n",
    "\n",
    "# Add comments on the performance in a cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_rpntvDZ8dKh"
   },
   "source": [
    "# Comments\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Experiment 1 (testing different vector sizes)\n",
    "In general, an increase in vector size should yield a higher classification accuracy. This is due to the effect of hashing (i.e. mapping each word in text onto a position in a vector); an increase in vector size leads to data being  more spread out and thus 'collisions' (which can create misclassifications) are less likely to occur.\n",
    "\n",
    "At vector size N = 3, every ML model in the experiment produced an training and testing accuracy of 83.40%.\n",
    "\n",
    "*   Logistic Regression - From the results above, LR yields the best performance out of the three ML models; At N = 30000, this model achieves 100.00% training accuracy and 97.20% testing accuracy. \n",
    "*   Naive Bayes - NB is the second best performing ML model; At N = 3000, this model achieves 97.40% training accuracy and 96.20% testing accuracy. One unusual observation is that the training and testing performance of this ML model drastically reduce at vector size N = 30000.\n",
    "*   Support Vector Machine - SVM is the worst perofrming ML model in the experiment; it yields a training and testing accuracy of 83.40% for all tested vector sizes. \n",
    "\n",
    "\n",
    "## Experiment 2 (testing different data types)\n",
    "From testing the four datatypes, lemmatisation and stop (i.e. removing stop words, for example 'I', 'the', 'a', etc.) produced equal training and testing results for the LR ML model; 100.00% training accuracy and 97.60% testing accuracy.  Note that \"Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\" ([wikipedia](https://en.wikipedia.org/wiki/Lemmatisation)). The NB model achieved equal training accuracy of 97.40% for bare and lemmatisation, although bare yielded the best testing accuracy (which was 96.20%). As in experiment 1, the SVM model achieved training and testing accuracy 83.40% for all four datatypes."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Axil Sudra Coursework-Part1-Tasks-v5.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
